{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TC3XkMetGWtK"
      },
      "source": [
        "# Neural Amp Modeler - Model Training Notebook\n",
        "ðŸ”¶**Before you run**ðŸ”¶\n",
        "\n",
        "Make sure to get a GPU! (From the upper-left menu, click Runtime->Change runtime type->Select \"GPU\" from the \"Hardware accelerator dropdown menu)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CQleTk7GJV8"
      },
      "source": [
        "## Step 1: Get data\n",
        "* **Download the reamp signal.** Here: [input.wav](https://drive.google.com/file/d/1KbaS4oXXNEuh2aCPLwKrPdf5KFOjda8G/view?usp=drive_link).\n",
        "* **Reamp your gear.** Then reamp the gear you want to model using it. Save that reamp as **\"output.wav\"**. *Note: Use 48kHz, 24-bit, mono.* For other sample rates, use [the CLI trainer](https://github.com/sdatkinson/neural-amp-modeler).\n",
        "* **Upload your files.** Upload `input.wav` and your `output.wav` by clicking the Folder icon on the left â¬… and then clicking the upload icon or by dragging the files into the panel."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7tRCyI_YjZjj"
      },
      "source": [
        "## Step 2: Train!\n",
        "Configure your training run below, then hit the Play button to start training!\n",
        "\n",
        "ðŸ•™NOTE: At default settings, training will take about 10 minutes.ðŸ•™"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "zrXbQY7vjZjk"
      },
      "outputs": [],
      "source": [
        "# Hi there! This is the code that makes this notebook work. Feel free to mess around\n",
        "# with it :)\n",
        "\n",
        "try:\n",
        "    import nam\n",
        "except ImportError as e:\n",
        "    print(\"Installing NAM into Colab. This should take a couple seconds.\")\n",
        "    # Check what we're starting with (Issue 399)\n",
        "    !if [ ! -d logs ]; then mkdir logs; fi\n",
        "    !pip list > logs/packages.log\n",
        "    # !pip install neural-amp-modeler==0.11 >& logs/install.log\n",
        "    # Hotfix\n",
        "    !pip install git+https://github.com/sdatkinson/neural-amp-modeler.git@hotfix-0.11.1 >& logs/install.log\n",
        "    # Hint: use the next line instead for the very latest!\n",
        "    # !pip install git+https://github.com/sdatkinson/neural-amp-modeler.git@main\n",
        "\n",
        "from typing import Optional\n",
        "\n",
        "from nam.train.colab import run\n",
        "from nam.models.metadata import GearType, ToneType, UserMetadata\n",
        "\n",
        "%load_ext tensorboard\n",
        "\n",
        "from functools import partial\n",
        "\n",
        "import ipywidgets as widgets\n",
        "\n",
        "# NOTE: Enums need to be handled carefully since the values need to be supplied literally here!\n",
        "\n",
        "#@markdown # Training parameters\n",
        "epochs = 100 #@param {type: \"number\"}\n",
        "architecture = \"standard\"  #@param [\"standard\", \"lite\", \"feather\", \"nano\"] {type: \"string\"}\n",
        "latency_samples = \"auto\"  #@param {type: \"string\"}\n",
        "ignore_checks = False #@param {type: \"boolean\"}\n",
        "\n",
        "#@markdown # Metadata\n",
        "use_metadata = False #@param {type: \"boolean\"}\n",
        "name = \"My model\" #@param {type:\"string\"}\n",
        "modeled_by = \"Your name\" #@param {type:\"string\"}\n",
        "gear_make = \"GearCo\" #@param {type:\"string\"}\n",
        "gear_model = \"GearName\" #@param {type:\"string\"}\n",
        "gear_type = \"amp\" #@param [\"amp\", \"pedal\", \"pedal_amp\", \"amp_cab\", \"amp_pedal_cab\", \"preamp\", \"studio\"] {type:\"string\"}\n",
        "tone_type = \"clean\" #@param [\"clean\", \"overdrive\", \"crunch\", \"hi_gain\", \"fuzz\"] {type:\"string\"}\n",
        "# DRY: \"(optional)\" used below in _parse_level_dbu\n",
        "reamp_default_value = \"(optional)\"\n",
        "reamp_send_level = \"(optional)\" #@param {type:\"string\"}\n",
        "reamp_return_level = \"(optional)\" #@param {type:\"string\"}\n",
        "#@markdown _For more information on measuring your reamp send and return calibration levels, see [the documentation](https://neural-amp-modeler.readthedocs.io/en/stable/tutorials/calibration.html)._\n",
        "\n",
        "# A few helper functions to make sense of the values provided above.\n",
        "def _verbose_enum(E, val):\n",
        "    try:\n",
        "        return E(val)\n",
        "    except ValueError as e:\n",
        "        raise ValueError(\n",
        "            str(e)\n",
        "            + \"\\nValid choices are: \"\n",
        "            + \", \".join(list(x.value for x in E))\n",
        "        )\n",
        "\n",
        "def _parse_latency(ls: str):\n",
        "    if ls.lower() == \"auto\":\n",
        "        return None\n",
        "    try:\n",
        "        return int(ls)\n",
        "    except ValueError as e:\n",
        "        raise ValueError(\n",
        "            f\"Invalid value for latency {ls} was given. Either use 'auto' or provide \"\n",
        "            f\"the reamp latency, in samples.\\nOriginal exception:\\n\\n{e}\"\n",
        "        )\n",
        "\n",
        "def _parse_level_dbu(value: str) -> Optional[float]:\n",
        "    if value in {\"\", reamp_default_value}:\n",
        "        return None\n",
        "    try:\n",
        "        return float(value)\n",
        "    except ValueError as e:\n",
        "        raise ValueError(\n",
        "            f\"Error parsing provided calibration level '{value}'. Either specify a \"\n",
        "            f\"number or leave blank.\\n\\nOriginal exception: {e}\"\n",
        "        )\n",
        "\n",
        "if not use_metadata:\n",
        "    print(\"Metadata will be skipped\")\n",
        "    user_metadata = None\n",
        "else:\n",
        "    print(\"Parsing user-provided metadata...\")\n",
        "    user_metadata = UserMetadata(\n",
        "        name=name,\n",
        "        modeled_by=modeled_by,\n",
        "        gear_make=gear_make,\n",
        "        gear_model=gear_model,\n",
        "        gear_type=_verbose_enum(GearType, gear_type.lower()),\n",
        "        tone_type=_verbose_enum(ToneType, tone_type.lower()),\n",
        "        input_level_dbu=_parse_level_dbu(reamp_send_level),\n",
        "        output_level_dbu=_parse_level_dbu(reamp_return_level),\n",
        "    )\n",
        "\n",
        "%tensorboard --logdir /content/lightning_logs\n",
        "run(\n",
        "    epochs=epochs,\n",
        "    architecture=architecture,\n",
        "    ignore_checks=ignore_checks,\n",
        "    user_metadata=user_metadata,\n",
        "    delay=_parse_latency(latency_samples)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "823KJ_L0Rchp"
      },
      "source": [
        "## Step 3: Check the results and download your model\n",
        "We're done!\n",
        "\n",
        "Have a look at the plot above to see how your model compares to the real gear you're modeling.\n",
        "Hopefully it looks good!\n",
        "Go to the file browser on the left panel â¬… and download `model.nam` from the `exported_model` directory (you may need to hit the refresh button).\n",
        "\n",
        "# ðŸŽ¸ **ENJOY!** ðŸŽ¸"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "nam",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "920df60c69944ba95f8c12adb41fedfdc8090c370a20d39253c7705973dd37db"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
